{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7078,
     "status": "ok",
     "timestamp": 1736264330583,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "3IUc-xYQao4a"
   },
   "outputs": [],
   "source": [
    "import os   # handling the files\n",
    "import pickle # storing numpy features\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # how much data is process till now\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input # extract features from image data.\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35246,
     "status": "ok",
     "timestamp": 1736264370876,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "BYHCZf1KassT",
    "outputId": "23bb63ed-f63d-4330-ce25-703931e01d5b"
   },
   "outputs": [],
   "source": [
    "! kaggle datasets download -d virajbagal/roco-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121294,
     "status": "ok",
     "timestamp": 1736264499375,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "NdUguf08azsM",
    "outputId": "15192c7f-bed3-468e-8c8b-d8f18adf0f66"
   },
   "outputs": [],
   "source": [
    "!unzip roco-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1736264507974,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "CICrNSu2a-SK"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/content/all_data/train/radiology'\n",
    "WORKING_DIR = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1736264561536,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "8dUhK-iNbd65",
    "outputId": "8022a382-5638-48f1-c312-e5a4b903dde7"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the CSV file\n",
    "input_csv_path = '/content/all_data/train/radiology/traindata.csv'\n",
    "# Path to the output text file\n",
    "output_txt_path = '/content/all_data/train/radiology/output.txt'\n",
    "\n",
    "# Open the CSV file and the output text file\n",
    "with open(input_csv_path, 'r') as csv_file, open(output_txt_path, 'w') as txt_file:\n",
    "    csv_reader = csv.DictReader(csv_file)  # Read the CSV file as a dictionary\n",
    "\n",
    "    for row in csv_reader:\n",
    "        name = row['name']  # Get the 'name' column\n",
    "        caption = row['caption']  # Get the 'caption' column\n",
    "\n",
    "        # Write to the text file in the format: name,caption\n",
    "        txt_file.write(f\"{name},{caption}\")\n",
    "\n",
    "print(f\"Name and caption have been written to {output_txt_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "executionInfo": {
     "elapsed": 7762,
     "status": "ok",
     "timestamp": 1736264582855,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "CgQYznm1bq5S",
    "outputId": "d7809ca6-ace2-4376-ab3e-657e729a3092"
   },
   "outputs": [],
   "source": [
    "# Load vgg16 Model\n",
    "model = VGG16()\n",
    "\n",
    "# restructure model\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "# Summerize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917,
     "referenced_widgets": [
      "1dbda40e3264441cb2c82b1605975eee",
      "288dee902197409894ec8cd38f5204b8",
      "3b2cb52f43044cb5acc5492297ea4907",
      "232db476e4e54e4282ccc5bc462c8e60",
      "df44dfb881ab47498bee76ce62ff9291",
      "9439651eb7f248c59c92507b35750097",
      "5b1196cf66684820ad34716261b96392",
      "ac7d50d5fb0f4397b02db28135d54b41",
      "7ee913a3fef644a29f4ed1334f6761a4",
      "0b5e9e1536334ddcba5eec0f82033d26",
      "8248975bbbec4514b655557c491293da"
     ]
    },
    "executionInfo": {
     "elapsed": 37027,
     "status": "ok",
     "timestamp": 1736264647117,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "G4mDeNnYbuYe",
    "outputId": "27390f68-b3f4-442c-9e2e-6706d61da091"
   },
   "outputs": [],
   "source": [
    "# extract features from image\n",
    "features = {}\n",
    "directory = os.path.join(BASE_DIR, 'images')\n",
    "\n",
    "count=0\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    if count==50:\n",
    "      break\n",
    "    count+=1\n",
    "    # load the image from file\n",
    "    img_path = directory + '/' + img_name\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocess image for vgg\n",
    "    image = preprocess_input(image)\n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    # get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # store feature\n",
    "    features[image_id] = feature\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1736264654661,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "8RDV0ax8b27I"
   },
   "outputs": [],
   "source": [
    "# store features in pickle\n",
    "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1736264657447,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "rSl6JixUcBt3"
   },
   "outputs": [],
   "source": [
    "# load features from pickle\n",
    "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1736264664961,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "7M6G-Kv3cCaB"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'output.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e504623894ed4c03a02a2139bc50427b",
      "932babe5d7b145fd84c6b9b02acadec0",
      "01f24343a6d146d293dfe0d22aa0e821",
      "1579debe88094d70b58edde5bf6e22d6",
      "ddf635b6162c471383b48ad1109051f2",
      "543ba089615b4030beade750fbdadd15",
      "73cce7719292484a97fb534dca81ba41",
      "39b75e2894f84ea18e48061ce812f503",
      "bbf20b9e41384fe88e5abfdfd45c015f",
      "88a41c3d52da4e8ea73a482c7aaf4a1a",
      "b69994101dae4631b60d92f461e2793b"
     ]
    },
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1736264680265,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "wXAHOwZscEO1",
    "outputId": "16b546dd-61aa-44f5-f89f-40709f60468f"
   },
   "outputs": [],
   "source": [
    "# create mapping of image to captions\n",
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    if image_id not in features.keys():\n",
    "      continue\n",
    "    # convert caption list to string\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1736264696486,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "KYyZ-4dRcH99",
    "outputId": "bd2957bf-f617-463a-e369-cc1a39f48c93"
   },
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1736264719372,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "8W1iZDfNcL7m"
   },
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # delete digits, special chars, etc.,\n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1736264762385,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "LwO1e1r2cRe4",
    "outputId": "d7f36f21-d2b2-42c1-a5ba-a525d29175cd"
   },
   "outputs": [],
   "source": [
    "# before preprocess of text\n",
    "mapping['PMC3639690_CRIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1736264777125,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "7oi4z6_RcT5T"
   },
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1736264785238,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "bu3-_PIncfmU",
    "outputId": "5fd045d6-8f85-4dc5-eca5-f047c4d1c208"
   },
   "outputs": [],
   "source": [
    "# before preprocess of text\n",
    "mapping['PMC3639690_CRIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1736264829530,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "W9Eo7uvmchlh"
   },
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1736264835308,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "ZbCPn2rFcsZ0",
    "outputId": "81fa9148-c1c1-4b2d-ebc8-0dcec3990883"
   },
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1736264843943,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "Nvj3N4bDctw1",
    "outputId": "d8a06266-463f-43d8-991e-a2f17ccdf7fd"
   },
   "outputs": [],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1736264853744,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "-4S68IMjcv7O"
   },
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1736264860913,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "i3e88g4TcyPM",
    "outputId": "37be8c31-4ba9-4b2b-c149-8582740eb8c6"
   },
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1736264868082,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "TQZJCOzic0Ev",
    "outputId": "64bc62d3-b2ac-40c6-87cf-e3c206d64013"
   },
   "outputs": [],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1736264876167,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "8JP1T6Kgc1y_"
   },
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "executionInfo": {
     "elapsed": 2182,
     "status": "ok",
     "timestamp": 1736264895065,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "CxYP5Ukic7gX",
    "outputId": "5675f700-137e-4eb6-d673-aae3f93cdbd5"
   },
   "outputs": [],
   "source": [
    "# encoder model\n",
    "# image feature layers\n",
    "\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5051,
     "status": "ok",
     "timestamp": 1736264913014,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "bznLFe5Ac3wV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_keys = list(mapping.keys())  # Keys for all images in the dataset\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_keys, val_keys = train_test_split(dataset_keys, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742324,
     "status": "ok",
     "timestamp": 1736266400590,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "_F2oiiEsc_pT",
    "outputId": "28054c78-6ee2-4d5d-9830-d3074bad539e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # Initialize storage\n",
    "    X1, X2, y = [], [], []\n",
    "    n = 0\n",
    "\n",
    "    while True:  # Infinite generator\n",
    "        for key in data_keys:\n",
    "            captions = mapping[key]\n",
    "            for caption in captions:\n",
    "                # Encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    # Collect inputs and outputs\n",
    "                    X1.append(features[key][0])  # Features\n",
    "                    X2.append(in_seq)  # Input sequence\n",
    "                    y.append(out_seq)  # Output sequence\n",
    "\n",
    "            # Yield batch when size matches batch_size\n",
    "            n += 1\n",
    "            if n == batch_size:\n",
    "                if len(X1) == 0:  # Skip empty batches\n",
    "                    continue\n",
    "                # Convert to TensorFlow tensors\n",
    "                X1_tensor = tf.convert_to_tensor(np.array(X1), dtype=tf.float32)\n",
    "                X2_tensor = tf.convert_to_tensor(np.array(X2), dtype=tf.float32)\n",
    "                y_tensor = tf.convert_to_tensor(np.array(y), dtype=tf.float32)\n",
    "\n",
    "                yield (X1_tensor, X2_tensor), y_tensor\n",
    "\n",
    "                # Reset storage\n",
    "                X1, X2, y = [], [], []\n",
    "                n = 0\n",
    "\n",
    "\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 4096), dtype=tf.float32),  # Features\n",
    "        tf.TensorSpec(shape=(None, max_length), dtype=tf.float32),  # Input sequences\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32),  # Output sequences\n",
    ")\n",
    "batch_size=32\n",
    "def safe_data_generator():\n",
    "    try:\n",
    "        yield from data_generator(train_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generator: {e}\")\n",
    "# Create a dataset from the generator\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    safe_data_generator,\n",
    "    output_signature=(\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, 4096), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, max_length), dtype=tf.float32),\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "steps_per_epoch = len(train_keys) // 32\n",
    "model.fit(train_dataset, epochs=200, steps_per_epoch=steps_per_epoch, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1736267047253,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "j0vFPcRNdE-8"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(WORKING_DIR+'/best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1736267054496,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "Lfj88i2JlJn5"
   },
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1736267062797,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "BgS2RdXilLnA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "def load_trained_model(model_path):\n",
    "    \"\"\"Load a pre-trained model from an .h5 file.\"\"\"\n",
    "    model = load_model(model_path,safe_mode=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1736267089445,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "sldSpBbIlNn2"
   },
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    model=keras.models.load_model(\"/content/best_model.keras\")\n",
    "    # add start tag for generation process\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "c82125bdaf2a48f48d6c55673093e28c",
      "44f8afcd326145f1bf3a79d280c5cf93",
      "18e1494fa37841dda7bdf6c238b55e66",
      "ebfa01035a19406dbb702530c2df8018",
      "6657c78fe68b4c2c82334410ee338e56",
      "8fd36212795c4309ae2c660eb5caa5ab",
      "98e6271fa9344e04b543bd3a7e5b7252",
      "8bfd6c60aa4e4da3a938fd974b6eece9",
      "aa19fa8256304bcaa0da43309c3356bd",
      "408bd379eb23480ea699ca498d19a1f3",
      "b7028b9ca1414bf4a3592ed74e3bdddf"
     ]
    },
    "executionInfo": {
     "elapsed": 101147,
     "status": "ok",
     "timestamp": 1736267202558,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "DemQ1yAolUJH",
    "outputId": "3921bdd4-2041-4ba9-ab79-c984e2046f89"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(train_keys):\n",
    "    # get actual caption\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwP5IQJ6mPla"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1736267246549,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "ZKgNR7BclXJu"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_caption(image_name):\n",
    "    # load the image\n",
    "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join(BASE_DIR, \"images\", image_name)\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1736267618318,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "y0DTC0Fsl6fq",
    "outputId": "2d4c2171-65d3-43aa-bdb1-d3f422cd908f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = '/content/all_data/train/radiology/images/'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory_path):\n",
    "    # List all files and directories\n",
    "    all_files = os.listdir(directory_path)\n",
    "\n",
    "    # Print the first 5 items\n",
    "    print(\"First 5 files or directories:\")\n",
    "    for item in all_files[:5]:  # Get the first 5\n",
    "        print(item)\n",
    "else:\n",
    "    print(f\"Directory not found: {directory_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1736267458642,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "Oiz1qud4mA5E",
    "outputId": "00da051b-4c9b-4532-b15d-becce6b6eb90"
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 4549,
     "status": "ok",
     "timestamp": 1736267709836,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "sMDt_iRwmuSS",
    "outputId": "9e5b22c6-5abb-4ad6-b19b-c65074c8c61f"
   },
   "outputs": [],
   "source": [
    "generate_caption(\"PMC3353704_DRJ-9-233-g004.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 3627,
     "status": "ok",
     "timestamp": 1736267749793,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "2QzIJ6SEnqmN",
    "outputId": "3ef72648-be38-4e19-c90a-afd87abd49c0"
   },
   "outputs": [],
   "source": [
    "generate_caption(\"PMC3639690_CRIM.EM2013-198617.003.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 2806,
     "status": "ok",
     "timestamp": 1736267803194,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "2Otv7awtn0kS",
    "outputId": "ae7714da-bca7-4c3f-fbe6-55d348e06269"
   },
   "outputs": [],
   "source": [
    "generate_caption(\"PMC5603107_CRIM2017-3531823.001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 3501,
     "status": "ok",
     "timestamp": 1736267879164,
     "user": {
      "displayName": "Sarvansh Mehta",
      "userId": "12717169397168865439"
     },
     "user_tz": -330
    },
    "id": "gAlC93CPn-f5",
    "outputId": "e0a8428e-971d-4093-bcba-eff6d93e9afa"
   },
   "outputs": [],
   "source": [
    "generate_caption(\"PMC3854574_10-1055-s-0033-1337123-i1200056-3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8v7BTYChoO04"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPo9teJSJQiQN0ZLSEkx3y/",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
